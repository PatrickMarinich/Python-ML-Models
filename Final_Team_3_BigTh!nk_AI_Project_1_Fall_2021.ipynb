{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PatrickMarinich/Python-ML-Models/blob/main/Final_Team_3_BigTh!nk_AI_Project_1_Fall_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slqeo3kMCFy2"
      },
      "source": [
        "# 1. Introduction\n",
        "\n",
        "Patrick Marinich and Matt Laddy\n",
        "\n",
        "**Welcome!**\n",
        "\n",
        "Hey, there! Welcome to your first project at BigTh!nk AI. You're going to build your own AI! There's some reading to do along the way, but it's going to be fun and worth it at the end!\n",
        "\n",
        "**Making a Copy**\n",
        "\n",
        "**Log into this file using your UMD email account!** This file is intended to be modified only **after** being copied! \n",
        "\n",
        "1. Go to \"File\" at the top left.\n",
        "2. Select \"Save a copy in Drive\".\n",
        "3. Open that copy, it should be in a new tab with the title \"Copy of BigTh!nk AI Project 1 Fall 2021\". Rename this to anything you want!\n",
        "4. Share your copy with your group members using \"Share\" at the top right.\n",
        "5. Move the colab file under your respective group's folder in the BigTh!nk shared drive in Fall 2021/Projects/Team X Colab files, where X is your group number. (For individuals, you'll see your name on the folder).\n",
        "\n",
        "**Asking For Help**\n",
        "\n",
        "During this whole thing, if you need any help at all, ping us on the BigTh!nk Discord server's #pyoneers channel with @AI Lead, @Python Lead, or @DS Lead. We'll be right with you.\n",
        "\n",
        "**Runtime Instructions**\n",
        "\n",
        "Remember, this environment divides all your code into cells.\n",
        "\n",
        "1. Connect to a Google server, top right. Wait for it to say \"Connected\".\n",
        "2. Go to \"Runtime\", top left. Select \"Change runtime type\" and set it to GPU.\n",
        "3. Click the `[ ]` on the left of each code cell to run it. Or just hit \"Runtime\" and select \"Restart and run all\" if you need.\n",
        "\n",
        "**Note:** If you run a cell, but modify something in a cell above it, then to avoid errors, you have to rerun every cell from and including the cell you modified.\n",
        "\n",
        "**Note:** If your syntax and tensor dimensions are correct but you're still getting an error, go to \"Runtime\" at the top left of this Colab and select \"Restart and run all\".\n",
        "\n",
        "**Google Drive Instructions**\n",
        "\n",
        "This will help us save your work.\n",
        "\n",
        "1. In the cell below, enter your group number in the `GROUP_NUMBER` variable, and run it. If you are an individual, enter the number we assigned you on the sign-up sheet next to your name.\n",
        "2. You will see a link starting with `https://accounts.google.com`. Click it. \n",
        "3. Log in using your UMD email account (on which you have the BigTh!nk AI Shared Drive). \n",
        "4. Click \"Allow\".\n",
        "5. Copy the displayed code into the box here under the code cell saying `Enter your authorization code:`, and press Enter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MACfF9Ti03g"
      },
      "source": [
        "## Previous coding exercises and slides\n",
        "\n",
        "You might find these helpful\n",
        "\n",
        "First Neural Net: https://colab.research.google.com/drive/1f-8IuR1O5pC6VdHgrmj3YMWCwihI5aoV?usp=sharing\n",
        "\n",
        "Image classification: https://colab.research.google.com/drive/1vz9niuT0KO1kLW7FzGi6i9qcBCM2N-Nv?usp=sharing\n",
        "\n",
        "Python practice: https://colab.research.google.com/drive/1pdqma3msfR55cKm2mg3znTdbKRAudOsG?usp=sharing\n",
        "\n",
        "Data Sci: https://colab.research.google.com/drive/1OeOtP5oMqJwgkw8UbUHdFdC-h5rKSQAx?usp=sharing\n",
        "\n",
        "OpenCV: https://colab.research.google.com/drive/1NUiyc3kn-zzuRyOjdLUj4fjk-rSkX60s?usp=sharing\n",
        "\n",
        "Link to all previous meeting slides: https://drive.google.com/drive/u/3/folders/1lWL8gBs4PwNyVuQrzvFCmcGa_4ETtK5Y \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m41vwMhNYwr9",
        "outputId": "5826c6f3-a180-41c0-829b-94251fc0f4a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Write your group's number below!\n",
        "GROUP_NUMBER = 3\n",
        "\n",
        "from google.colab import drive # ADD TEXT ABOVE\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCPtRSYYCncL"
      },
      "source": [
        "Here are some libraries you'll see very often in machine learning and deep learning. Remember, importing TensorFlow implicitly imports Keras! Go ahead and run this cell below with the imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSEdLt2JCiMQ",
        "outputId": "b984132c-3b9b-496e-aa53-9d342ea873df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (4.0.1)\n",
            "Collecting tensorflow_datasets\n",
            "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (2.23.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (5.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (4.62.3)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (21.2.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets) (1.19.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow_datasets) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-datasets-4.4.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf # our favorite framework for machine learning!\n",
        "# Note that keras is imported by default, so we'll often call tf.keras.<command>\n",
        "\n",
        "import math\n",
        "import numpy as np # the fundamental building block of ML: arrays!\n",
        "import matplotlib.pyplot as plt # this will help us plot and visualize our data\n",
        "import logging\n",
        "import seaborn as sns # this will help us in understanding performance metrics towards the end\n",
        "\n",
        "!pip install -U tensorflow_datasets # we're getting the repository of datasets that this project's dataset belongs to\n",
        "\n",
        "import tensorflow_datasets as tfds \n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CosKXZX1hif"
      },
      "source": [
        "# 2. The Problem\n",
        "\n",
        "**Issue**\n",
        "\n",
        "It's 2051 and there's a new malaria outbreak! People in Yew Nork City are shivering in fever and chills! It's up to you, the AI engineers at BigTh!nk BioTech (established after the famous coronavirus pandemic of 2020) to speed up diagnoses.\n",
        "\n",
        "With patients storming hospitals, we need a way to make testing incredibly rapid and accurate - and the current methods are far too slow. We need you, and we need the power of AI.\n",
        "\n",
        "**Task**\n",
        "\n",
        "Your job is to make a model that can learn from previous images to identify a malaria-infected cell. By the end of this project, given an image of a cell, your model's prediction must be one of two outputs:\n",
        "\n",
        "0: Uninfected\n",
        "\n",
        "1: Parasitized\n",
        "\n",
        "**Supervised Learning**\n",
        "\n",
        "Remember, a supervised learning AI will look at many input examples (images), look at what the output actually was (labels) for those input examples, and then try to figure out what the link is. \n",
        "\n",
        "Once it learns that, it will try to predict the output of an unseen, previously unknown input datapoint (a new image).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdsxmAWha0QB"
      },
      "source": [
        "\n",
        "\n",
        " 3. The Raw Data\n",
        "\n",
        "The hospitals of Yew Nork City have compiled 27,558 cell images from thin blood smear slide images. If you're not sure what that is, don't worry! It's just a way to take pictures of microscopic cells. These pictures are what you'll train your algorithm on to identify which ones are parasitized.\n",
        "\n",
        "Go ahead and run these code cells below to load the dataset and divide it into a training dataset, `train_ds`, and a test dataset, `test_ds`.\n",
        "\n",
        "Note that this dataset is only made up of one \"split\" called \"train\" on tensorflow. A \"split\" is basically how a dataset is distributed. The fashion MNIST dataset had two splits called \"train\" and \"test\" which made our job easier for us. \n",
        "\n",
        "The syntax to slice part of the split is as follows: `train[:num%]` or `train[num%:]`. Look at [this](https://stackoverflow.com/questions/509211/understanding-slice-notation) resource for help on slicing in Python\n",
        "\n",
        "You can choose any split ratio you think is good.\n",
        "\n",
        "There are some additional parameters here that you have to think about setting for this particular problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uadsz-CYQSDs",
        "outputId": "4a87046d-bffd-42b3-c92e-a4722172fae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mDownloading and preparing dataset 337.08 MiB (download: 337.08 MiB, generated: Unknown size, total: 337.08 MiB) to /root/tensorflow_datasets/malaria/1.0.0...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Start coding here!\n",
        "\n",
        "# We will use these variables for visualization of the dataset later.\n",
        "\n",
        "ds, info = tfds.load('malaria',split='train',shuffle_files=False,with_info=True)\n",
        "\n",
        "\n",
        "\n",
        "# We will use these variables for actually training the model.\n",
        "train_ds, test_ds = tfds.load(\n",
        "  'malaria',\n",
        "  split = ['train[:65%]', 'train[65%:]'],\n",
        "  shuffle_files= False, as_supervised = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_MuUJAv8njd"
      },
      "source": [
        "We want to know exactly how many images are in each of our two datasets! Fill in the blanks appropriately to get this number and then print it out. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roPK6j63_NDu"
      },
      "outputs": [],
      "source": [
        "NUM_TRAIN_IMAGES = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "\n",
        "\n",
        "NUM_TEST_IMAGES = tf.data.experimental.cardinality(test_ds).numpy()\n",
        "#prints the number train then test\n",
        "print(NUM_TRAIN_IMAGES)\n",
        "print(NUM_TEST_IMAGES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDxfvjpZFvo9"
      },
      "source": [
        "Running this cell below will show you some examples of raw, unprocessed images of our patients' cells and their labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZngaAAUFz4R"
      },
      "outputs": [],
      "source": [
        "vis = tfds.visualization.show_examples(ds, info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZUGYacrHUqy"
      },
      "source": [
        "Go take another look at the labels of the images above. The labels in this raw dataset are unintuitive: currently, the labels are:\n",
        "\n",
        "0: parasitized\n",
        "\n",
        "1: uninfected\n",
        "\n",
        "\"*A tragedy! Not intuitive at all! Where is the concern?*\" - unknown source\n",
        "\n",
        "We're going to invert these in **4. Processing the Data**, so that the labels will *later* be:\n",
        "\n",
        "0: uninfected\n",
        "\n",
        "1: parasitized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uElDbDCcWjKH"
      },
      "source": [
        "If you take yet another look at the pictures above, you might notice the images aren't evenly sized.\n",
        "\n",
        "Running the cell below can show you an example of what we mean by uneven image size.\n",
        "\n",
        "If the output is something like:\n",
        "\n",
        "`Image size: (140, 120, 3)`\n",
        "\n",
        "it just means the image is 140 x 120 pixels, and the 3 tells you how many color channels are in the image. There are 3 here, which are red, green, and blue (RGB). Pixel stuff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh7kpyCBNVNT"
      },
      "outputs": [],
      "source": [
        "for image, label in train_ds.take(2):\n",
        "    print(\"Image size: \", image.numpy().shape)\n",
        "    print(\"Label: \", label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfETY0BBWo2z"
      },
      "source": [
        "# 4. Processing the Data\n",
        "\n",
        "If you're supposed to build an AI to learn well from these images, you need some cleaner data! It makes it easier on whatever model you build to learn. \n",
        "\n",
        "This cell defines a couple of standard constants we want, as well as 3 functions we've written to clean the data. It'll call those functions on `train_ds` and `test_ds` to create `clean_train_ds` and `clean_test_ds`.\n",
        "\n",
        "The functions here, however, are incomplete. Fill in the blanks using what we know! Remember the goal is to:\n",
        "\n",
        "1. Resize images to 200 by 200 pixels \n",
        "\n",
        "2. Invert the labels\n",
        "\n",
        "[This](https://www.tensorflow.org/api_docs/python/tf/image/resize_with_crop_or_pad) documentation might be useful for Step 1 :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MiGn4K3OP8y"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = [200, 200]\n",
        "\n",
        "\n",
        "# DO NOT MODIFY THIS FUNCTION\n",
        "def convert(image, label):\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  return image, label\n",
        "\n",
        "\n",
        "# resizing each image to 200 x 200\n",
        "def pad(image,label): \n",
        "  image,label = convert(image, label)\n",
        "  image = tf.image.resize_with_crop_or_pad(image,200,200)\n",
        "  return image, label\n",
        "\n",
        "# switching the 0 and 1 around, as mentioned\n",
        "def invert_labels(image, label):\n",
        "\n",
        "#Possible Solution\n",
        "  if label == 1:\n",
        "    label = 0 ;\n",
        "  else:\n",
        "    label=1\n",
        "  return image, label\n",
        "\n",
        "\n",
        "# DO NOT MODIFY CODE BELOW\n",
        "clean_train_ds = (\n",
        "    train_ds\n",
        "    .map(pad)\n",
        "    .map(invert_labels)\n",
        ")\n",
        "\n",
        "clean_test_ds = (\n",
        "    test_ds\n",
        "    .map(pad)\n",
        "    .map(invert_labels)\n",
        ") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOncpLwhXHri"
      },
      "source": [
        "# 5. The Clean Data\n",
        "\n",
        "Run this cell below and look how clean these examples are! All the cell pictures have been padded and fit into images that are 200 x 200 pixels. These are ready to be read by your model.\n",
        "\n",
        "As you might have noticed in the cell above, the names of the clean training and testing datasets are `clean_train_ds` and `clean_test_ds`. We're going to use Matplotlib to visualize our changes to the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbf9tDSVO6Hk"
      },
      "outputs": [],
      "source": [
        "image_batch, label_batch = next(iter(clean_train_ds.batch(BATCH_SIZE)))\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "    plt.figure(figsize = (10, 10))\n",
        "    for n in range(25):\n",
        "        ax = plt.subplot(5, 5, n+1)\n",
        "        plt.imshow(image_batch[n])\n",
        "        if label_batch[n]:\n",
        "            plt.title(\"parasitized (1) \")\n",
        "        else:\n",
        "            plt.title(\"uninfected (0) \")\n",
        "        plt.axis(\"off\")\n",
        "show_batch(image_batch.numpy(), label_batch.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plskras1H0ar"
      },
      "source": [
        "Looks like the labels are indeed fixed! 0 now means uninfected, and 1 now means parasitized.\n",
        "\n",
        "Don't worry about this little cell below, just run it. We're just getting the data in batches to be sent to the model for training and testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNEvRK9AHZ1K"
      },
      "outputs": [],
      "source": [
        "clean_train_ds = clean_train_ds.repeat().shuffle(NUM_TRAIN_IMAGES).batch(BATCH_SIZE)\n",
        "clean_test_ds = clean_test_ds.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wpIAdDNbu8v"
      },
      "source": [
        "# 6. The Model\n",
        "\n",
        "Phew! We're finally here. Your job is to create and optimize a machine learning model to identify malaria in a cell. As a re-statement: the model should produce 0 on an uninfected cell image and 1 on a parasitized cell image.\n",
        "\n",
        "**Convolutional Neural Networks (CNNs)**\n",
        "\n",
        "To detect malaria, we need some way to *see* the cells! In computer vision, this is commonly achieved through CNNs. A flashlight in the dark, seeing part by part until it pieces the picture together.\n",
        "\n",
        "We're writing the convolutional part of the network. **Don't change it!** \n",
        "\n",
        "**Dense (Hidden) Layers**\n",
        "\n",
        "Write your part of the model under `# BUILD YOUR PART HERE`. Remember that we are dealing with image data and need to construct the layers accordingly. Think back to the exercises!\n",
        "`model.summary()` will give you a good idea of your finished network.\n",
        "\n",
        "**Output**\n",
        "\n",
        "Think about how many neurons and which activation function you need for this layer. Remember, this problem is a binary classification problem (only two outputs, 0 or 1) and we discussed the output layer for these problems in meetings!\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "Remember, *parameters* are what the model adjusts by itself. *Hyperparameters* are what *you* can build and adjust in the model to maximize performance. In this project, you're free to change any or all of the following:\n",
        "\n",
        "\n",
        "\n",
        "*   Depth of neural network (number of dense layers except output)\n",
        "*   Width of neural network (number of neurons per dense layer except output)\n",
        "*   Activation function of each dense layer (except output)\n",
        "\n",
        "You *can* also change the following, but we'll get to these two in **7. Compiling the Model**.\n",
        "\n",
        "*   Optimization algorithm\n",
        "*   Learning rate\n",
        "\n",
        "\n",
        "\n",
        "Go save Yew Nork City!\n",
        "\n",
        "No pressure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woSRjYs66pBg"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             \n",
        "    # CNN: this is the convolutional part of the neural network, we got this\n",
        "    # DO NOT MODIFY THE CNN\n",
        "    \n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu, input_shape = (200, 200, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
        "\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), padding = 'same', activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2), strides = 2),\n",
        "\n",
        "    # BUILD YOUR PART HERE (Dense and output layers):\n",
        "\n",
        "    ###\n",
        "    ### where we get to experiment \n",
        "    ###\n",
        "    tf.keras.layers.Flatten(),\n",
        "   #input layer needs 200x200 nodes = 40000,\n",
        "    tf.keras.layers.Dense(256,activation= 'tanh'),\n",
        "    tf.keras.layers.Dense(128,activation= 'tanh'),\n",
        "    tf.keras.layers.Dense(64,activation= 'tanh'),\n",
        "    tf.keras.layers.Dense(64,activation= 'tanh'),\n",
        "\n",
        "    tf.keras.layers.Dense(1,activation = 'sigmoid'),\n",
        "   \n",
        "  \n",
        " \n",
        "\n",
        "])\n",
        "\n",
        "model.summary() # this is going to print a quick little summary of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxZRmu14dsmW"
      },
      "source": [
        "# 7. Compiling the Model\n",
        "\n",
        "There are three parts to this function: `optimizer`, `loss` function, and `metrics`. We're going to explain them all.\n",
        "\n",
        "**Optimizer**\n",
        "\n",
        "You are free to set an optimizer you think works best! You can find a list of [Keras optimizers here](https://keras.io/api/optimizers/) if you scroll down to the \"Available Optimizers\" heading.\n",
        "\n",
        "Remember, setting an optimizer can be done either by passing a string, like \n",
        "\n",
        "`optimizer = 'RMSProp'`\n",
        "\n",
        "or by calling a function from `tf.keras`. For example, \n",
        "\n",
        "`optimizer = tf.keras.optimizers.RMSProp()`. \n",
        "\n",
        "We recommend the latter, since you can set `learning_rate` as a parameter of the function, like \n",
        "\n",
        "`optimizer = tf.keras.optimizers.RMSProp(learning_rate = 0.1)`\n",
        "\n",
        "(Hint: don't just settle for RMSProp!)\n",
        "\n",
        "**Note:** If you do not explicitly set a `learning_rate` value, it'll go by the default value listed in the documentation. Default `learning_rate` values can be found on that optimizer's documentation ([click here](https://keras.io/api/optimizers/)).\n",
        "\n",
        "**Loss function**\n",
        "\n",
        "The model should use the `BinaryCrossentropy()` loss function, commonly used in binary classifiers. The model adjusts its parameters (weights and biases) to minimize this loss. **Don't change this!**\n",
        "\n",
        "Remember, the lower the loss, the better!\n",
        "\n",
        "**Metrics**\n",
        "\n",
        "Loss is fine to understand if the model is learning. But in the real world, a common way to test is called the **confusion matrix**. We'll get to this in **9. Performance on Test Data**, but basically, it has four metrics we calculate, given by the bullets below. And you can check this link for all possible [performance metrics on Keras](https://keras.io/api/metrics/). \n",
        "Here are the metrics that will help us measure your models' performance.\n",
        "\n",
        "*   `TruePositives` (guess correct, guessed 1)\n",
        "*   `TrueNegatives` (guess correct, guessed 0)\n",
        "*   `FalsePositives` (guess wrong, guessed 1)\n",
        "*   `FalseNegatives` (guess wrong, guessed 0)\n",
        "\n",
        "In order to get these values from our model, add the following to the metrics array that you pass in: `tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgzBYF4CCLhk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#optmizer can change later, and can change learining rate\n",
        "optimizerIn = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#dont change loss\n",
        "lossIn = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "#compilier\n",
        "model.compile(optimizer = optimizerIn,\n",
        "              loss=lossIn,\n",
        "              metrics = [tf.keras.metrics.TruePositives(), tf.keras.metrics.TrueNegatives(), tf.keras.metrics.FalsePositives(), tf.keras.metrics.FalseNegatives()])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deOFRVVabz9R"
      },
      "source": [
        "# 8. Training\n",
        "\n",
        "**Epochs and Batches**\n",
        "\n",
        "When you read a chapter in a book, that's basically a *batch*. When you read the book end to end once, that's basically an *epoch*. \n",
        "\n",
        "The same way, when a model reads a group of (in this case) 32 examples, it's a batch. That's `BATCH_SIZE = 32`, which we defined at the start of **4. Processing the Data**. When a model reads through all of the training examples once, it's an epoch. That's `NUMBER_OF_EPOCHS = 5` here, meaning 5 epochs. **Don't change `NUMBER_OF_EPOCHS`!**\n",
        "\n",
        "`steps_per_epoch` is just a calculation of how many batches there are per epoch.\n",
        "\n",
        "**Beginning Training**\n",
        "\n",
        "Use the appropriate function from Keras which will begin to train the model and tweak its parameters (weights and biases). In addition to passing in your clean training set, you also have to set the number of epochs, and the steps_ per_epoch (number of training images divided by batch size).\n",
        "\n",
        "Run this cell below to begin the actual training process for the model. You will see a progress bar, followed by the `loss` value and the number of `true_positives`, `true_negatives`, `false_negatives`, and `false_positives` in that epoch.\n",
        "\n",
        "Training can take around 3-4 minutes on the GPU setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjOFgFxkHjT0"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "steps = math.ceil(NUM_TRAIN_IMAGES/BATCH_SIZE)\n",
        "print(steps)\n",
        "# YOUR CODE GOES HERE\n",
        "\n",
        "\n",
        "model.fit(clean_train_ds, epochs=NUMBER_OF_EPOCHS, batch_size=BATCH_SIZE, steps_per_epoch=steps)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD9DlgcN0YY-"
      },
      "source": [
        "Great! You were able to train your model! \n",
        "\n",
        "**Saving the Model**\n",
        "\n",
        "After 5 epochs, your model has tuned its weights and biases in a certain configuration. To save that model in the BigTh!nk AI shared drive, just run the cell below. Don't edit it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdkZDPycRvKr"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/Shared drives/BigTh!nk AI/Fall 2021/Projects/F21 Models/%s.h5' % GROUP_NUMBER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI-GXLIlYJPv"
      },
      "source": [
        "# 9. Performance on Test Data\n",
        "\n",
        "Loss is fine, but it doesn't show us the whole picture. And looking at the confusion matrix as numbers doesn't *show* us what's going on. So we're going to actually calculate the accuracy of your model from a visualized confusion matrix!\n",
        "\n",
        "To do that, first we need to test your model on data that it hasn't yet seen or trained on. Remember when we split the dataset into `train_ds` and `test_ds`? We're testing on the cleaned version of `test_ds` which we named `clean_test_ds`. Test data is also called *unseen data*.\n",
        "\n",
        "**Beginning Testing**\n",
        "\n",
        "Run your model on the test data with this cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is_Otm66TQSu"
      },
      "outputs": [],
      "source": [
        "test_loss, test_tp, test_tn, test_fp, test_fn = model.evaluate(clean_test_ds, steps = math.ceil(NUM_TEST_IMAGES/BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv5jdg6iCKjC"
      },
      "source": [
        "Using the Seaborn library we imported, we're going to draw the confusion matrix for your model to assess its performance on the test data!\n",
        "\n",
        "Run these 2 cells to build the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEDTHXqYgtg6"
      },
      "outputs": [],
      "source": [
        "def draw_confusion_matrix(tp, tn, fp, fn):\n",
        "  cf_matrix = np.array([[tp, fp], [fn, tn]])\n",
        "  group_names = ['True Pos','False Pos','False Neg','True Neg']\n",
        "  group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
        "  group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "  labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
        "  labels = np.asarray(labels).reshape(2,2)\n",
        "  sns.heatmap(cf_matrix, annot = labels, fmt = '', cmap = 'Blues', xticklabels = False, yticklabels = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgItVziBhPl2"
      },
      "outputs": [],
      "source": [
        "draw_confusion_matrix(test_tp, test_tn, test_fp, test_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKVlYgxNJQw9"
      },
      "source": [
        "Now, we can actually calculate the accuracy from the confusion matrix. Remember,\n",
        "\n",
        "**Accuracy = (Total Trues) / (Total Predictions)**\n",
        "\n",
        "Run the cell below to see the accuracy of your model on the test data!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTaNRECFpVAu"
      },
      "outputs": [],
      "source": [
        "accuracy = (test_tp + test_tn) / (test_tp + test_tn + test_fp + test_fn)\n",
        "\n",
        "print(\"The accuracy of your model is %.7f, or about %d%%.\" % (accuracy, round(accuracy*100)))\n",
        "if (accuracy > 0.9):\n",
        "  print(\"Great job! Your high-accuracy AI is about to save Yew Nork City in style.\")\n",
        "elif (accuracy > 0.8):\n",
        "  print(\"Not bad! Your model is going to do a pretty good job of helping Yew Nork City.\")\n",
        "else:\n",
        "  print(\"Good effort, but Yew Nork City needs a slightly more reliable model in order to diagnose patients accurately.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Kn8ZIQCp5J"
      },
      "source": [
        "# 10. Good job!\n",
        "\n",
        "Well, you did it. If your model did better than 80%, BigTh!nk's AI engineers really mean business. Yew Nork City is eternally indebted to your service to its ailing civilians!\n",
        "\n",
        "If you couldn't quite manage 80%, no worries. You can always make a copy of this project and tweak even more things to optimize your model even more. Go ahead and disable the cell where you saved your model, right above **9. Performance on Test Data**, to make infinite tweaks privately without overwriting your group's model. \n",
        "\n",
        "You can also go through the specific data processing functions in detail, if you'd like. \n",
        "\n",
        "Here are some documentation links if you want to learn more about the libraries, functions, and tools you (knowingly or unknowingly) used in this project!\n",
        "\n",
        "*   [Malaria Dataset](https://www.tensorflow.org/datasets/catalog/malaria)\n",
        "*   [TensorFlow](https://www.tensorflow.org/guide)\n",
        "*   [Keras](https://keras.io/api/)\n",
        "*   [Seaborn](https://seaborn.pydata.org/)\n",
        "*   [Numpy](https://numpy.org/)\n",
        "*   [Matplotlib](https://matplotlib.org/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Final Team 3 BigTh!nk AI Project 1 Fall 2021.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}